---
layout: post
title: Introduction to Gaussian Processes
---


Gaussian Processes (GPs) are a really flexible, really powerful class of models that have seen a lot of use in the last fifteen or twenty years. In this post I'd like to go through the background of these models, what they are, and how they work; primarily so I can try straighten out some of these ideas in my own head.

This is the first of what I reckon will be three posts, and here I'll really just go through some simple ideas in model fitting, gearing up for a discussion of GPs.

### Regression

In regression problems our aim is to try to predict some target variable, $y$, given some input, $x$. Let's assume that there is some hidden function which we want to model, and let's begin with the univariate case, $f: \mathbb{R} \rightarrow \mathbb{R}$. We have some training data, $\mathbf{x} = [x_1,\dots,x_n]^T$ and $\mathbf{y} = [y_1,\dots,y_n]^T$ where $y_i = f(x_i)$, and we want to make predictions of $y$ at some new points $x_*$.

The best place to start is at the beginning with everyone's favourite, a linear model

$$y = mx + c$$

If we assume that the hidden function generating our data is linear, then the easiest case is if we have two data points. Given two observations we can solve the resultant pair of simultaneous equations, and get this

![2pts](/images/gp/2points.png)

Cool.

Things get a lot worse if we add another point that doesn't sit on the line defined by the other two. Now we have an overdetermined system of equations, 3 equations with 2 unknowns, so there are 3 lines we can fit

![3pts](/images/gp/3points.png)

Which line is the 'correct' one? This was a real problem for people like Gauss, Laplace, and Legendre, the first scientists concerned with trying to fit models to observation.[^1] Sticking with the assumption that our data truly comes from a linear function then the reason that our points don't sit on the same line is that there are some un-modeled sources of variance in our observational set-up. To account for this we add another term to our model which explains the discrepancy,

$$
y_i = mx_i + c + \epsilon_i
$$

Now we have $N+2$ parameters, we have an $\epsilon$ for each data point along with $m$ and $c$. We get around this by making another assumption, that $\epsilon$ is a random variable drawn from some probability distribution. Often we assume that $\epsilon$ is drawn from a normal distribution with some mean and standard deviation,[^2]

$$
\epsilon \sim \mathcal{N}(\mu,\, \sigma^2)
$$

Now we can solve the equations (by some method) and find the line of best fit to these points. It's important to bear in mind the distinction between model and algorithm here. What we have done is provide a model, the process we use to generate the line of best fit (least squares, for example) is the algorithm.


 I think it's actually more instructive to look at things from the other perspective, what if we only had one observation? In that case we're left with an underdetermined system, we now have one equation in two unknowns, $m$ and $c$.

All is not lost though, we can still fit a linear model, provided we are willing to make some assumptions about our parameters. For instance, let's assume that the intercept is distributed according to a normal distribution. This is very similar to what we did in the overdetermined case, we substitute some variable with a probability distribution, though the variables have very different interpretations -- in the overdetermined case the variable we introduce is a kind of noise model, in this case it's one of our parameters. Based on this assumption we can sample intercepts from the probability distribution and fit possible lines to our point

```python
px, py = np.random.rand(2)
x = np.linspace(0, 1, 100)

for i in range(100):
        c = np.random.randn()
        m = (py - c) / px
        y = m * x + c
        plt.plot(x, y, c='0.8', alpha=0.7)
```

![1pt](/images/gp/fit_point.png)

I'm a big fan of this model. It's fairly transparent, and it agrees well with our intuitions -- if we make some assumption about one of our parameters and combine that assumption with our data we can generate some confidence about where our true function lies, we can be really confident near the observation and less so further away. It's nice to see that in a plot.

Now that we're putting probability distributions over the parameters of our model we're straying into Bayesian territory. We can begin to look at linear models for which we have some prior belief about the parameters, let's stick with the linear model and normal distributions, and we can update our beliefs using Bayes rule after having observed some data

$$
P(c \mid y) = \frac{P(y \mid c) \, P(c)}{P(y)}
$$

So our posterior probability distribution, the probability of $c$ given the data, is the probability of the data given the $c$ (the likelihood), times the prior, and divided by the marginal likelihood (to normalise).

The nice thing here is that since we have begun with a Gaussian prior and our likelihood is Gaussian, and the product of Gaussians is Gaussian,[^3] then our posterior will be Gaussian and we can analytically solve for its mean and covariance. This is exceptionally convenient.

All this has been by the way of motivating the background to GPs, though I've managed to do it without really mentioning what they are. What I was aiming to convey here was was why someone might try to do Bayesian regression, and in the next post I'll begin disussing how you can use GPs to extend these ideas.


---

[^1]: Gauss's work on this is really fascinating, definitely worth reading up on. He was trying to determine the parameters of the orbits of [comets](https://www.schillerinstitute.org/fid_97-01/982_orbit_ceres.pdf), trying to reconcile a set of measurements and find the most probable ellipsoidal orbit. It's actually in this [work](https://archive.org/details/bub_gb_ORUOAAAAQAAJ) that he introduced the normal distribution.

[^2]: This assumption means that least squares estimators are also the maximum likelihood estimators, which is nice.

[^3]: This fact, along with the fact that the sum of Gaussians is a Gaussian, explains almost everything you need to know about GPs.
